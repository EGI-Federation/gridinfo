#!/usr/bin/python
##############################################################################
# Copyright (c) Members of the EGEE Collaboration. 2004.
# See http://www.eu-egee.org/partners/ for details on the copyright
# holders.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at #
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS
# OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
##############################################################################
#
# NAME :        gstat-update-rrd
#
# DESCRIPTION : This script updates the rrd from the snapshot database
#
# AUTHORS :     Laurence.Field@cern.ch
#               Joanna@twgrid.org
#
# EXAMPLE :     ./gstat-update-rrd
#
##############################################################################

import os
import sys
import time
import logging
from datetime import datetime
DIR = os.path.dirname(__file__)
APPS_DIR = os.path.join(DIR, '..','apps')
sys.path.append(APPS_DIR)
os.environ['DJANGO_SETTINGS_MODULE']='core.settings'
from glue.models import gluece, gluese, gluesubcluster, gluevoview
from core.utils import *

modulename='topology.models'
__import__(modulename)
module=sys.modules[modulename]
start_time=datetime.now()

# Set logging
log = logging.getLogger(sys.argv[0])
hdlr = logging.StreamHandler(sys.stderr)
formatter = logging.Formatter('[%(levelname)s]: %(message)s')
hdlr.setFormatter(formatter)
log.addHandler(hdlr)
log.setLevel(2)

# Welcome
log.info('Welcome to the snapshot script')
log.info('Processing started ...')

data_dir='/var/cache/gstat/rrd'


def createDir(dir_path):
    # check if path exists. if not, create
    if (not os.path.exists(dir_path)):
        os.makedirs(dir_path)
            
def createArchive(archive_dir, archive_path, archive_type, archive_period, data_names):
    """If archive is not present, create new RRD archive

    Name of archive is: rrd/node_type/node_uniqueid/dataname.rrd
    Historic data is kept for 1000 sample periods of:
        5min    ~3.5 days
        30min   ~20.8 days  
        2hr     ~83.3 days
        1day    1000 days
    For each period the data is conslidated into MIN, MAX and AVG
    """

    # create archive directory if not exists
    createDir(archive_dir)

    # check if archive exists. if not, create
    if(not os.path.exists(archive_path)):
        create_cmd = "rrdtool create %s" % (archive_path)
        for data_name in data_names:
            create_cmd += \
            " DS:%s:%s:%s:U:U" % (data_name, archive_type, archive_period*2)
        create_cmd += \
            " RRA:AVERAGE:0.5:1:1000"+\
            " RRA:AVERAGE:0.5:6:1000"+\
            " RRA:AVERAGE:0.5:24:1000"+\
            " RRA:AVERAGE:0.5:288:1000"
            
        os.system(create_cmd)
        #print "create RRD: "+create_cmd+"\nResults: "+query_res

def archiveData(archive_path, data_list):
    """Archives data for long term storage

    """

    # update archive with detailed data
    # CAUTION! make sure how many datasources they have in their database, 
    # and in what order these were defined
    update_cmd = "rrdtool update %s N" %(archive_path)
    for data in data_list:
        update_cmd += ":%s" %(data)
    os.system(update_cmd)
    #print "updated RRD: "+update_cmd+"\nResults: "+query_res


# -----------------------------------------
# -- GlueSE storage space size archiving --
# -----------------------------------------
sub_data_dir='%s/SE' %(data_dir)
createDir(sub_data_dir)

distinct = {}
objects = gluese.objects.all()
for obj in objects:
    if (not distinct.has_key(obj.uniqueid)):
        obj_unique_data_dir = '%s/%s' %(sub_data_dir, str(obj.uniqueid).replace('/',''))
        
        datasources = {'online':   ['totalonlinesize', 'usedonlinesize'],
                       'nearline': ['totalnearlinesize', 'usednearlinesize']}
        
        for key in datasources.keys():
            obj_data_file = '%s/%s.rrd' %(obj_unique_data_dir, key)
            
            # Create RRD file
            createArchive(archive_dir    = obj_unique_data_dir, 
                          archive_path   = obj_data_file, 
                          archive_type   = 'GAUGE', 
                          archive_period = 600, 
                          data_names     = datasources[key])
        
            # Store RRD data
            data_list = []
            for attr in datasources[key]:
                data = obj.__getattribute__(attr)
                if (data == "unset" or data == ""):
                    data = "0"
                data_list.append(data)
            #print obj_data_file, data
            archiveData(archive_path = obj_data_file, 
                        data_list    = data_list)
            distinct[obj.uniqueid] = None
            
# ------------------------------------------
# -- GlueSubCluster CPU numbers archiving --
# ------------------------------------------
sub_data_dir='%s/SubCluster' %(data_dir)
createDir(sub_data_dir)

distinct = {}
objects = gluesubcluster.objects.all()

for obj in objects:
    if (not distinct.has_key(obj.uniqueid)):
        obj_unique_data_dir = '%s/%s' %(sub_data_dir, str(obj.uniqueid).replace('/',''))
        
        datasources = {'cpu':   ['physicalcpus', 'logicalcpus']}
        
        for key in datasources.keys():
            obj_data_file = '%s/%s.rrd' %(obj_unique_data_dir, key)
            
            # Create RRD file
            createArchive(archive_dir    = obj_unique_data_dir, 
                          archive_path   = obj_data_file, 
                          archive_type   = 'GAUGE', 
                          archive_period = 600, 
                          data_names     = datasources[key])
        
            # Store RRD data
            data_list = []
            for attr in datasources[key]:
                data = obj.__getattribute__(attr)
                if (data == "unset" or data == ""):
                    data = "0"
                data_list.append(data)
            #print obj_data_file, data
            archiveData(archive_path = obj_data_file, 
                        data_list    = data_list)            
            distinct[obj.uniqueid] = None


# -------------------------------------------------
# -- GlueVOView job numbers archiving (VO-based) --
# -------------------------------------------------
sub_data_dir='%s/VO' %(data_dir)
createDir(sub_data_dir)

# Create CE to Cluster mapping
ce_to_cluster_mapping= {}
objects = gluece.objects.all()
for object in objects:
    if ( not ce_to_cluster_mapping.has_key(object.uniqueid) ):
         ce_to_cluster_mapping[object.uniqueid] = object.gluecluster_fk

# Create VO to VOViewMapping
vo_to_view_mapping = {}
objects = gluemultivalued.objects.filter(attribute='GlueCEAccessControlBaseRule')
for object in objects:
    if ( object.localid == "" ):
        continue
    
    if ( object.value[:3] == "VO:" ):
        vo = object.value[3:]
    if ( object.value[:3] == "VOMS:" ):
        vo = object.value[5:]
        index = vo.find("\\")
        vo = vo[:index]
    if ( not vo_to_view_mapping.has_key(object.uniqueid) ):
        vo_to_view_mapping[object.uniqueid] = {}
    vo_to_view_mapping[object.uniqueid][object.localid] = vo

data = {}
datasources = {'job':   ['totaljobs', 'runningjobs', 'waitingjobs']}
# Store Total, Running, Waiting Jobs
objects = gluevoview.objects.all()
for view in objects:
    try:
        vo = vo_to_view_mapping[view.glueceuniqueid][view.localid]
    except KeyError, e:
        #print "Could not get vo mapping for %s %s" %(view.glueceuniqueid, view.localid)
        continue

    try:
        cluster = ce_to_cluster_mapping[str(view.glueceuniqueid)]
    except KeyError, e:
        #print "Could not get cluster mapping for %s" %(view.glueceuniqueid)
        continue

    if ( not data.has_key(cluster) ):
        data[cluster] = {}
    if ( not data[cluster].has_key(vo) ):
        data[cluster][vo] = [0,0,0]
    
    for attr in datasources['job']:
        value = view.__getattribute__(attr)
        if (attr == "waitingjobs" and value == "444444"): value = "0"
        if (value == "unset" or value == ""):             value = "0"
        data[cluster][vo][datasources['job'].index(attr)] += convert_to_integer(value)    
        
for cluster in data.keys():
    for vo in data[cluster].keys():
        try:
            obj_unique_data_dir = '%s/%s/%s' %(sub_data_dir, 
                                               str(vo).replace('/',''), 
                                               str(cluster).replace('/',''))
            
            for key in datasources.keys():
                obj_data_file = '%s/%s.rrd' %(obj_unique_data_dir, key)
                
                # Create RRD file
                createArchive(archive_dir    = obj_unique_data_dir, 
                              archive_path   = obj_data_file, 
                              archive_type   = 'GAUGE', 
                              archive_period = 600, 
                              data_names     = datasources[key])
            
                # Store RRD data
                data_list = data[cluster][vo]
                archiveData(archive_path = obj_data_file, 
                            data_list    = data_list)   
            
        except KeyError:
          pass

"""
# Create GlueCE to GlueCluster mapping
glueces = {}
objects = getGlueEntity('gluece', all=True)
for obj in objects:
    if ( not glueces.has_key(obj.uniqueid) ):
         glueces[obj.uniqueid] = obj.gluecluster_fk


# We will make an assumption that localid=VO/Role
objects = getGlueEntity('gluevoview', all=True)

datasources = {'job':   ['totaljobs', 'runningjobs', 'waitingjobs']}
jobs = {}
for obj in objects:
    try:
        key = (str(obj.localid), str(glueces[obj.gluece_fk]))
        if key not in jobs.keys():
            jobs[key] = {}
            for attr in datasources['job']:
                data = obj.__getattribute__(attr)
                if (attr == "waitingjobs" and data == "444444"):
                    data = "0"
                if (data == "unset" or data == ""):
                    data = "0"
                jobs[key][attr] = convert_to_integer(data)
        else:
            for attr in datasources['job']:
                data = obj.__getattribute__(attr)
                if (attr == "waitingjobs" and data == "444444"):
                    data = "0"
                if (data == "unset" or data == ""):
                    data = "0"
                jobs[key][attr] += convert_to_integer(data)       
    except KeyError:
        pass
        
for jobskey in jobs.keys():
    try:
        obj_unique_data_dir = '%s/%s/%s' %(sub_data_dir, 
                                           str(jobskey[0]).replace('/',''), 
                                           str(jobskey[1]).replace('/',''))
        for key in datasources.keys():
            obj_data_file = '%s/%s.rrd' %(obj_unique_data_dir, key)
            
            # Create RRD file
            createArchive(archive_dir    = obj_unique_data_dir, 
                          archive_path   = obj_data_file, 
                          archive_type   = 'GAUGE', 
                          archive_period = 600, 
                          data_names     = datasources[key])
        
            # Store RRD data
            data_list = []
            for attr in datasources[key]:
                data = jobs[jobskey][attr]
                data_list.append(data)
            #print obj_data_file, data
            archiveData(archive_path = obj_data_file, 
                        data_list    = data_list)   
    except KeyError:
        pass
        #print "No mapping found"
"""

# Get time taken for the archiving
log.info("Time taken: %s" % str(datetime.now() - start_time))


